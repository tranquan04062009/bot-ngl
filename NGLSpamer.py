import telegram
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ConversationHandler, CallbackContext
import requests
import time
import random
import os
from fake_useragent import UserAgent

API_URL_NGL = "https://ngl.link/api/submit"
TOKEN_BOT_TELEGRAM = "7766543633:AAFnN9tgGWFDyApzplak0tiJTafCxciFydo"

UA = UserAgent()
USER_AGENTS = [UA.chrome, UA.firefox, UA.safari, UA.opera, UA.internetexplorer, UA.random]
REFERERS = [
    "https://ngl.link/",
    "https://www.google.com/",
    "https://www.facebook.com/",
    "https://www.twitter.com/",
    "https://www.youtube.com/",
    "https://www.instagram.com/",
    "https://www.youtube.com/"
]
ACCEPT_ENCODINGS = ["gzip", "deflate", "br", "*"]
LANGUAGES = ["vi-VN", "vi", "en-US", "en", "ja-JP", "ja", "ko-KR", "ko", "zh-CN", "zh"]
PLATFORMS = ["Windows", "macOS", "Linux", "Android", "iOS"]
BROWSERS = ["Chrome", "Firefox", "Safari", "Opera", "Internet Explorer", "Edge"]
CONNECTIONS = ["keep-alive", "close"]
CACHE_DIRECTIVES = ["max-age=0", "no-cache", "no-store", "must-revalidate", "proxy-revalidate"]

(
    NHAP_LINK_NGL,
    NHAP_TIN_NHAN_SPAM,
    NHAP_SO_LAN_SPAM,
    XAC_NHAN_PROXY,
    UPLOAD_FILE_PROXY
) = range(5)

phuong_phap_spam = [
    "van_ban_thong_thuong", "them_emoji", "tien_to_ky_tu_ngau_nhien", "hau_to_ky_tu_ngau_nhien",
    "hon_hop_chu_hoa_thuong", "leet_speak", "thay_the_ky_tu_dac_biet", "lap_lai_ky_tu",
    "dao_lon_tu_ngu", "thay_the_tu_dong_nghia", "mo_rong_cau", "rut_gon_cau",
    "bien_doi_cau_hoi", "bien_doi_cau_cam_than", "giong_dieu_mia_mai", "giong_dieu_tich_cuc",
    "giong_dieu_tieu_cuc", "giong_dieu_khan_cap", "giong_dieu_to_mo", "khoa_truong",
    "noi_giam", "cau_hoi_tu_tu", "phep_loi_suy_nghi", "an_du", "so_sanh",
    "nhan_hoa", "gi·ªÖu_c·ª£t", "choi_chu", "lap_am_dau", "lap_nguyen_am",
    "mo_phong_am_thanh", "bao_truoc", "hoi_tuong", "nghi_nghich_ly", "cau_tuong_phan",
    "phan_de", "cao_trao", "ha_trao", "cham_lua", "danh_ngon",
    "uy·ªÉn_ng·ªØ", "c√°ch_noi_khinh_thuong", "c√°ch_noi_thiet_che", "hoan_du", "·∫©n_d·ª•",
    "ƒëi·ªáp_ng·ªØ_ƒë·∫ßu_c√¢u", "ƒëi·ªáp_ng·ªØ_cu·ªëi_c√¢u", "ƒë·∫£o_ng·ªØ", "c√¢u_d·∫´n_li√™n_ti·∫øp", "ƒëa_li√™n_t·ª´",
    "t·ªânh_l∆∞·ª£c_li√™n_t·ª´"
]

def ap_dung_phuong_phap_spam(tin_nhan, phuong_phap):
    if phuong_phap == "van_ban_thong_thuong": return tin_nhan
    elif phuong_phap == "them_emoji":
        emojis = ["üòÇ", "ü§£", "üî•", "üíØ", "üöÄ", "‚ú®", "üåü", "üéâ", "üéä", "üéà"]
        return tin_nhan + " " + random.choice(emojis)
    elif phuong_phap == "tien_to_ky_tu_ngau_nhien":
        tien_to = ''.join(random.choice("abcdefghijklmnopqrstuvwxyz0123456789") for _ in range(random.randint(3, 7)))
        return tien_to + " " + tin_nhan
    elif phuong_phap == "hau_to_ky_tu_ngau_nhien":
        hau_to = ''.join(random.choice("abcdefghijklmnopqrstuvwxyz0123456789") for _ in range(random.randint(3, 7)))
        return tin_nhan + " " + hau_to
    elif phuong_phap == "hon_hop_chu_hoa_thuong":
        return "".join(char.upper() if random.random() < 0.5 else char.lower() for char in tin_nhan)
    elif phuong_phap == "leet_speak":
        leet_chars = {'a': '4', 'e': '3', 'i': '1', 'o': '0', 's': '5', 't': '7', 'l': '1', 'z': '2'}
        return "".join(leet_chars.get(char.lower(), char) for char in tin_nhan)
    elif phuong_phap == "thay_the_ky_tu_dac_biet":
        symbol_chars = {'a': '@', 'b': '8', 'e': '‚Ç¨', 'g': '6', 'h': '#', 'i': '!', 'l': '¬£', 'o': '()', 's': '$', 't': '+', 'z': '2'}
        return "".join(symbol_chars.get(char.lower(), char) for char in tin_nhan)
    elif phuong_phap == "lap_lai_ky_tu":
        if len(tin_nhan) > 3:
            index = random.randint(1, len(tin_nhan) - 2)
            char_to_repeat = tin_nhan[index]
            repeat_count = random.randint(2, 5)
            return tin_nhan[:index] + char_to_repeat * repeat_count + tin_nhan[index+1:]
        return tin_nhan
    elif phuong_phap == "dao_lon_tu_ngu":
        words = tin_nhan.split()
        if len(words) > 2:
            random.shuffle(words[1:-1])
            return " ".join(words)
        return tin_nhan
    elif phuong_phap == "thay_the_tu_dong_nghia":
        synonyms = {
            "t·ªët": ["tuy·ªát v·ªùi", "xu·∫•t s·∫Øc", "phi th∆∞·ªùng", "tuy·ªát di·ªáu"],
            "x·∫•u": ["t·ªìi t·ªá", "kh·ªßng khi·∫øp", "kinh kh·ªßng", "gh√™ s·ª£"],
            "vui": ["h·∫°nh ph√∫c", "sung s∆∞·ªõng", "m√™ ly", "ph·∫•n kh·ªüi"]
        }
        words = tin_nhan.split()
        new_words = []
        for word in words:
            lower_word = word.lower()
            if lower_word in synonyms:
                new_words.append(random.choice(synonyms[lower_word]))
            else:
                new_words.append(word)
        return " ".join(new_words)
    elif phuong_phap == "mo_rong_cau":
        expansions = [
            "Th·ª±c t·∫ø l√†, ", "Tr√™n th·ª±c t·∫ø, ", "Th√†nh th·∫≠t m√† n√≥i, ", "ƒê·ªÉ t√¥i n√≥i cho b·∫°n bi·∫øt, ", "Tin hay kh√¥ng t√πy b·∫°n, "
        ]
        if len(tin_nhan.split()) < 10:
            return random.choice(expansions) + tin_nhan
        return tin_nhan
    elif phuong_phap == "rut_gon_cau":
        contractions = [
            "V·ªÅ c∆° b·∫£n, ", "N√≥i m·ªôt c√°ch ƒë∆°n gi·∫£n, ", "T√≥m l·∫°i, ", "ƒê·ªÉ t·ªïng k·∫øt, "
        ]
        if len(tin_nhan.split()) > 15:
            return random.choice(contractions) + tin_nhan
        return tin_nhan
    elif phuong_phap == "bien_doi_cau_hoi":
        if not tin_nhan.endswith("?"):
            return tin_nhan + "?"
        return tin_nhan
    elif phuong_phap == "bien_doi_cau_cam_than":
        if not tin_nhan.endswith("!"):
            return tin_nhan + "!"
        return tin_nhan
    elif phuong_phap == "giong_dieu_mia_mai":
        sarcastic_phrases = ["nh∆∞ th·ªÉ r·ªìi", "·ª´ ƒë√∫ng r·ªìi", "ch·∫Øc ch·∫Øn r·ªìi", "r√µ r√†ng", "t√¥i kh√¥ng nghƒ© v·∫≠y"]
        return tin_nhan + ", " + random.choice(sarcastic_phrases)
    elif phuong_phap == "giong_dieu_tich_cuc":
        positive_phrases = ["Tuy·ªát v·ªùi!", "ƒê·ªânh!", "Phi th∆∞·ªùng!", "Tuy·ªát di·ªáu!", "Xu·∫•t s·∫Øc!"]
        return tin_nhan + " " + random.choice(positive_phrases)
    elif phuong_phap == "giong_dieu_tieu_cuc":
        negative_phrases = ["Th·∫≠t kh√¥ng may", "ƒê√°ng bu·ªìn thay", "Ti·∫øc l√†", "Th·∫≠t x·∫•u h·ªï khi", "Qu√° t·ªá"]
        return random.choice(negative_phrases) + ", " + tin_nhan
    elif phuong_phap == "giong_dieu_khan_cap":
        urgent_phrases = ["Nhanh l√™n!", "Mau l√™n!", "Ngay l·∫≠p t·ª©c!", "ƒê·ª´ng ch·∫≠m tr·ªÖ!", "H√†nh ƒë·ªông ngay!"]
        return random.choice(urgent_phrases) + " " + tin_nhan
    elif phuong_phap == "giong_dieu_to_mo":
        curiosity_phrases = ["ƒêo√°n xem?", "B·∫°n c√≥ bi·∫øt kh√¥ng?", "S·ª± th·∫≠t th√∫ v·ªã:", "Nghe n√†y:", "B·∫°n s·∫Ω kh√¥ng tin ƒë√¢u:"]
        return random.choice(curiosity_phrases) + " " + tin_nhan
    elif phuong_phap == "khoa_truong":
        hyperboles = ["t·ªët nh·∫•t t·ª´ tr∆∞·ªõc ƒë·∫øn nay", "ƒëi·ªÅu tuy·ªát v·ªùi nh·∫•t", "ngo√†i s·ª©c t∆∞·ªüng t∆∞·ª£ng", "kh√≥ tin", "kinh ng·∫°c"]
        return tin_nhan + ", n√≥ l√† " + random.choice(hyperboles) + "!"
    elif phuong_phap == "noi_giam":
        understatements = ["kh√¥ng t·ªá l·∫Øm", "·ªïn th√¥i", "t·∫°m ƒë∆∞·ª£c", "kh√¥ng ph·∫£i t·ªá nh·∫•t", "c√≥ th·ªÉ ch·∫•p nh·∫≠n ƒë∆∞·ª£c"]
        return tin_nhan + ", " + random.choice(understatements)
    elif phuong_phap == "cau_hoi_tu_tu":
        questions = ["ƒê√∫ng kh√¥ng?", "B·∫°n kh√¥ng nghƒ© v·∫≠y sao?", "Kh√¥ng ph·∫£i sao?", "ƒê·ªìng √Ω?", "B·∫°n bi·∫øt m√†?"]
        return tin_nhan + ", " + random.choice(questions) + " ü§î"
    elif phuong_phap == "phep_loi_suy_nghi":
        analogies = [", nh∆∞ m·ªôt...", ", t∆∞∆°ng t·ª± nh∆∞ m·ªôt...", ", nh∆∞ th·ªÉ n√≥ l√† m·ªôt...", ", n√≥ gi·ªëng nh∆∞ m·ªôt..."]
        nouns = ["t√™n l·ª≠a", "d√≤ng s√¥ng", "ng·ªçn n√∫i", "ƒë·∫°i d∆∞∆°ng", "c∆°n b√£o", "gi·∫•c m∆°", "b√†i h√°t"]
        return tin_nhan + analogies[random.randint(0, len(analogies)-1)] + " " + random.choice(nouns)
    elif phuong_phap == "an_du":
        metaphors = ["m·ªôt...", "m·ªôt...", "c·ªßa...", "trong..."]
        nouns = ["ng·ªçn h·∫£i ƒëƒÉng hy v·ªçng", "bi·ªÉn kh·ªï", "v∆∞·ªùn h·∫°nh ph√∫c", "c∆°n b√£o tranh c√£i", "d√≤ng s√¥ng th·ªùi gian"]
        return tin_nhan + ", n√≥ l√† " + random.choice(nouns)
    elif phuong_phap == "so_sanh":
        similes = ["nh∆∞...", "nh∆∞... nh∆∞...", "t∆∞∆°ng t·ª± nh∆∞...", "gi·ªëng v·ªõi..."]
        comparisons = ["m·ªôt ng√¥i sao s√°ng", "m·ªôt con s∆∞ t·ª≠ g·∫ßm r√∫", "m·ªôt l√†n gi√≥ nh·∫π", "m·ªôt ng·ªçn l·ª≠a d·ªØ d·ªôi", "m·ªôt c√°i b√≥ng im l·∫∑ng"]
        return tin_nhan + ", nh∆∞ " + random.choice(comparisons)
    elif phuong_phap == "nhan_hoa":
        personifications = ["Gi√≥ th√¨ th·∫ßm", "M·∫∑t tr·ªùi m·ªâm c∆∞·ªùi", "M∆∞a kh√≥c", "Th·ªùi gian tr√¥i ƒëi", "S·ª± im l·∫∑ng h√©t l√™n"]
        return random.choice(personifications) + ", " + tin_nhan.lower()
    elif phuong_phap == "gi·ªÖu_c·ª£t":
        ironic_phrases = ["√îi, th·∫≠t tuy·ªát v·ªùi", "Qu√° ho√†n h·∫£o", "Ch√≠nh x√°c nh·ªØng g√¨ t√¥i mu·ªën", "Kh√¥ng th·ªÉ t·ªët h∆°n", "Tin tuy·ªát v·ªùi"]
        return random.choice(ironic_phrases) + ", " + tin_nhan
    elif phuong_phap == "choi_chu":
        puns = ["C·∫£i b·∫Øp l·∫≠t c·ªß c·∫£i ƒë∆∞·ªùng!", "B·∫°n c√≥ vui khi g·∫∑p t√¥i kh√¥ng?", "T√¥i th√≠ch ƒë·∫≠u n√†nh b·∫°n!", "Th·ªùi gian tr√¥i nhanh nh∆∞ m·ªôt m≈©i t√™n; ru·ªìi gi·∫•m th√≠ch chu·ªëi.", "B·∫°n g·ªçi m·ªôt con c√° kh√¥ng c√≥ m·∫Øt l√† g√¨? Fsh!"]
        return tin_nhan + ". " + random.choice(puns)
    elif phuong_phap == "lap_am_dau":
        words = tin_nhan.split()
        if len(words) >= 2:
            first_letter = words[0][0].lower()
            alliterative_words = [w for w in words[1:] if w[0].lower() == first_letter]
            if alliterative_words:
                return words[0] + " " + " ".join(alliterative_words) + " v√† " + " ".join([w for w in words[1:] if w[0].lower() != first_letter])
        return tin_nhan
    elif phuong_phap == "lap_nguyen_am":
        vowels = "aeiou"
        words = tin_nhan.split()
        if len(words) >= 2:
            vowel_sound = ""
            for char in words[0].lower():
                if char in vowels:
                    vowel_sound = char
                    break
            if vowel_sound:
                assonated_words = [w for w in words[1:] if vowel_sound in w.lower()]
                if assonated_words:
                    return words[0] + " v√† " + " ".join(assonated_words) + " " + " ".join([w for w in words[1:] if vowel_sound not in w.lower()])
        return tin_nhan
    elif phuong_phap == "mo_phong_am_thanh":
        onomatopoeic_words = ["Bang!", "Boom!", "Clang!", "Hiss!", "Buzz!", "Woof!", "Meow!", "Sizzle!", "Splash!", "Whisper!"]
        return random.choice(onomatopoeic_words) + " " + tin_nhan
    elif phuong_phap == "bao_truoc":
        foreshadowing_phrases = ["H·ªç ƒë√¢u bi·∫øt r·∫±ng...", "Kh√¥ng ai hay bi·∫øt...", "M·∫ßm m·ªëng tai h·ªça ƒë√£ ƒë∆∞·ª£c gieo...", "S·ªë ph·∫≠n ƒë√£ c√≥ nh·ªØng k·∫ø ho·∫°ch kh√°c...", "M·ªôt c∆°n b√£o ƒëang –Ω–∞–∑—Ä–µ–≤–∞–ª–∞..."]
        return random.choice(foreshadowing_phrases) + " " + tin_nhan
    elif phuong_phap == "hoi_tuong":
        flashback_phrases = ["Nhi·ªÅu nƒÉm tr∆∞·ªõc...", "Ng√†y x∆∞a...", "Ng√†y x·ª≠a ng√†y x∆∞a...", "Trong m·ªôt k√Ω ·ª©c xa xƒÉm...", "T·ª´ s√¢u th·∫≥m qu√° kh·ª©..."]
        return random.choice(flashback_phrases) + ", " + tin_nhan
    elif phuong_phap == "nghi_nghich_ly":
        paradoxes = ["S·ª± im l·∫∑ng th·∫≠t ƒëi·∫øc tai.", "√çt h∆°n l√† nhi·ªÅu h∆°n.", "T√¥i ph·∫£i t√†n nh·∫´n ƒë·ªÉ t·ª≠ t·∫ø.", "K·∫ª ng·ªëc kh√¥n ngoan.", "V·ª´a ng·ªçt v·ª´a ƒë·∫Øng."]
        return tin_nhan + ". " + random.choice(paradoxes)
    elif phuong_phap == "cau_tuong_phan":
        oxymorons = ["ng·ªçt ƒë·∫Øng", "x√°c s·ªëng", "im l·∫∑ng ƒëi·∫øc tai", "t√¥m jumbo", "h·ªón lo·∫°n c√≥ t·ªï ch·ª©c"]
        return tin_nhan + ", nh∆∞ m·ªôt " + random.choice(oxymorons)
    elif phuong_phap == "phan_de":
        antitheses = ["Ng∆∞·ªùi t√≠nh kh√¥ng b·∫±ng tr·ªùi t√≠nh.", "L·ªùi n√≥i l√† b·∫°c, im l·∫∑ng l√† v√†ng.", "D·ªÖ ƒë·∫øn d·ªÖ ƒëi.", "Nh√¢n v√¥ th·∫≠p to√†n.", "H√£y l·∫Øng nghe t·∫•t c·∫£ m·ªçi ng∆∞·ªùi, nh∆∞ng ch·ªâ n√≥i v·ªõi s·ªë √≠t."]
        return tin_nhan + ". " + random.choice(antitheses)
    elif phuong_phap == "cao_trao":
        climax_phrases = ["v√† ƒëi·ªÅu quan tr·ªçng nh·∫•t l√†...", "tr√™n h·∫øt...", "ƒëi·ªÉm m·∫•u ch·ªët l√†...", "ƒë·ªânh cao c·ªßa t·∫•t c·∫£...", "k·∫øt th√∫c ho√†nh tr√°ng l√†..."]
        return tin_nhan + ", " + random.choice(climax_phrases)
    elif phuong_phap == "ha_trao":
        anticlimaxes = ["nh∆∞ng sau ƒë√≥ kh√¥ng c√≥ g√¨ x·∫£y ra.", "v√† n√≥ kh√° l√† g√¢y th·∫•t v·ªçng.", "v√† n√≥ ch·ªâ... ·ªïn th√¥i.", "nh∆∞ng h√≥a ra n√≥ h∆°i g√¢y th·∫•t v·ªçng.", "v√† ti·∫øt l·ªô l·ªõn l√†... meh."]
        return tin_nhan + ", " + random.choice(anticlimaxes)
    elif phuong_phap == "cham_lua":
        return tin_nhan + "..."
    elif phuong_phap == "danh_ngon":
        epigrams = ["Sai l·∫ßm l√† c·ªßa con ng∆∞·ªùi, nh∆∞ng ƒë·ªÉ th·ª±c s·ª± l√†m r·ªëi tung m·ªçi th·ª© c·∫ßn ƒë·∫øn m√°y t√≠nh.", "T√¥i c√≥ th·ªÉ c∆∞·ª°ng l·∫°i m·ªçi th·ª© tr·ª´ s·ª± c√°m d·ªó.", "C√°ch duy nh·∫•t ƒë·ªÉ lo·∫°i b·ªè s·ª± c√°m d·ªó l√† ƒë·∫ßu h√†ng n√≥.", "H√£y l√† ch√≠nh m√¨nh; m·ªçi ng∆∞·ªùi kh√°c ƒë√£ c√≥ ng∆∞·ªùi kh√°c l√†m r·ªìi.", "S·ª± th·∫≠t hi·∫øm khi thu·∫ßn khi·∫øt v√† kh√¥ng bao gi·ªù ƒë∆°n gi·∫£n."]
        return tin_nhan + ". " + random.choice(epigrams)
    elif phuong_phap == "uy·ªÉn_ng·ªØ":
        euphemisms = ["qua ƒë·ªùi", "kh√≥ khƒÉn v·ªÅ kinh t·∫ø", "khuy·∫øt t·∫≠t", "c∆° s·ªü c·∫£i hu·∫•n", "ƒë√£ qua s·ª≠ d·ª•ng"]
        original_words = ["ch·∫øt", "ngh√®o", "t√†n t·∫≠t", "t√π", "ƒë√£ d√πng"]
        word_pairs = list(zip(original_words, euphemisms))
        for original, euphemism in word_pairs:
            tin_nhan = tin_nhan.replace(original, euphemism, 1)
        return tin_nhan
    elif phuong_phap == "c√°ch_noi_khinh_thuong":
        litotes_phrases = ["kh√¥ng t·ªá", "kh√¥ng ph·∫£i kh√¥ng gi·ªëng", "kh√¥ng ph·∫£i kh√¥ng ƒë√°ng k·ªÉ", "kh√¥ng ph·∫£i kh√¥ng ph·ªï bi·∫øn", "kh√¥ng ph·∫£i m·ªôt ch√∫t"]
        return tin_nhan + ", n√≥ " + random.choice(litotes_phrases)
    elif phuong_phap == "c√°ch_noi_thiet_che":
        meiosis_phrases = ["ch·ªâ l√† m·ªôt v·∫øt x∆∞·ªõc", "h∆°i h∆°i", "m·ªôt ch√∫t x√≠u", "m·ªôt s·ª± c·ªë nh·ªè", "m·ªôt ch√∫t b·∫•t ti·ªán"]
        return tin_nhan + ", n√≥ ch·ªâ l√† " + random.choice(meiosis_phrases)
    elif phuong_phap == "hoan_du":
        synecdoches = ["b√°nh xe", "s·ª£i ch·ªâ", "gi√†y ·ªëng tr√™n m·∫∑t ƒë·∫•t", "b·ªô com l√™", "v∆∞∆°ng mi·ªán"]
        full_meanings = ["xe h∆°i", "qu·∫ßn √°o", "binh l√≠nh", "doanh nh√¢n", "ch·∫ø ƒë·ªô qu√¢n ch·ªß"]
        word_pairs = list(zip(synecdoches, full_meanings))
        for synecdoche, full_meaning in word_pairs:
            tin_nhan = tin_nhan.replace(full_meaning, synecdoche, 1)
        return tin_nhan
    elif phuong_phap == "·∫©n_d·ª•":
        metonymies = ["Nh√† Tr·∫Øng", "Ph·ªë Wall", "Hollywood", "Thung l≈©ng Silicon", "Ph·ªë Fleet"]
        full_meanings = ["T·ªïng th·ªëng M·ªπ", "c√°c t·ªï ch·ª©c t√†i ch√≠nh", "ng√†nh c√¥ng nghi·ªáp ƒëi·ªán ·∫£nh M·ªπ", "ng√†nh c√¥ng nghi·ªáp c√¥ng ngh·ªá M·ªπ", "b√°o ch√≠ Anh"]
        word_pairs = list(zip(metonymies, full_meanings))
        for metonymy, full_meaning in word_pairs:
            tin_nhan = tin_nhan.replace(full_meaning, metonymy, 1)
        return tin_nhan
    elif phuong_phap == "ƒëi·ªáp_ng·ªØ_ƒë·∫ßu_c√¢u":
        prefixes = ["H√£y nh·ªõ r·∫±ng, ", "ƒê·ª´ng bao gi·ªù qu√™n, ", "H√£y ghi nh·ªõ, ", "Lu√¥n xem x√©t, ", "Ch√∫ng ta ƒë·ª´ng b·ªè qua, "]
        return random.choice(prefixes) + tin_nhan
    elif phuong_phap == "ƒëi·ªáp_ng·ªØ_cu·ªëi_c√¢u":
        suffixes = [", h√£y nh·ªõ ƒëi·ªÅu ƒë√≥.", ", h√£y ghi nh·ªõ ƒëi·ªÅu ƒë√≥.", ", ƒë·ª´ng bao gi·ªù qu√™n.", ", lu√¥n xem x√©t ƒëi·ªÅu ƒë√≥.", ", ch√∫ng ta ƒë·ª´ng b·ªè qua ƒëi·ªÅu ƒë√≥."]
        return tin_nhan + random.choice(suffixes)
    elif phuong_phap == "ƒë·∫£o_ng·ªØ":
        words = tin_nhan.split()
        if len(words) >= 4:
            return words[0] + " " + words[1] + " " + words[-2] + " " + words[-1]
        return tin_nhan
    elif phuong_phap == "c√¢u_d·∫´n_li√™n_ti·∫øp":
        verbs = ["chinh ph·ª•c", "ph√° h·ªßy", "x√¢y d·ª±ng", "t·∫°o ra", "m·∫•t m√°t"]
        nouns1 = ["tr√°i tim", "t√¢m tr√≠", "v·∫≠n may", "ƒë·∫ø ch·∫ø", "gi·∫•c m∆°"]
        nouns2 = ["th√†nh ph·ªë", "qu·ªëc gia", "v∆∞∆°ng qu·ªëc", "th·∫ø gi·ªõi", "thi√™n h√†"]
        return random.choice(verbs).capitalize() + " " + random.choice(nouns1) + " v√† " + random.choice(nouns2) + "."
    elif phuong_phap == "ƒëa_li√™n_t·ª´":
        words = tin_nhan.split()
        if len(words) >= 3:
            return " v√† ".join(words)
        return tin_nhan
    elif phuong_phap == "t·ªânh_l∆∞·ª£c_li√™n_t·ª´":
        words = tin_nhan.split()
        if len(words) >= 3:
            return ", ".join(words[:-1]) + " " + words[-1]
        return tin_nhan
    else:
        return tin_nhan

def spam_ngl(ngl_link, tin_nhan, so_lan_spam, proxies=None):
    print(f"B·∫Øt ƒë·∫ßu spam NGL: {ngl_link}, Tin nh·∫Øn: '{tin_nhan}', S·ªë l·∫ßn: {so_lan_spam}, Proxies: {bool(proxies)}")
    dem_spam_thanh_cong = 0
    dem_bi_chan = 0
    retry_delay = 2
    max_retries = 5
    for i in range(so_lan_spam):
        phuong_phap = random.choice(phuong_phap_spam)
        tin_nhan_spam = ap_dung_phuong_phap_spam(tin_nhan, phuong_phap)

        du_lieu = {
            "link": ngl_link.split("/")[-1],
            "question": tin_nhan_spam
        }

        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Referer': random.choice(REFERERS),
            'Origin': 'https://ngl.link',
            'Accept-Language': random.choice(LANGUAGES),
            'Accept-Encoding': random.choice(ACCEPT_ENCODINGS),
            'Connection': random.choice(CONNECTIONS),
            'Cache-Control': random.choice(CACHE_DIRECTIVES),
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
        }

        proxy = None
        if proxies:
            proxy_str = random.choice(proxies)
            if proxy_str:
                proxy_host, proxy_port = proxy_str.split(":")
                proxy = {'http': f'http://{proxy_host}:{proxy_port}', 'https': f'https://{proxy_host}:{proxy_port}'}

        retry_count = 0
        while retry_count < max_retries:
            try:
                start_time = time.time()
                response = requests.post(API_URL_NGL, data=du_lieu, headers=headers, proxies=proxy, timeout=15)
                response.raise_for_status()
                end_time = time.time()
                response_time = end_time - start_time

                if response.status_code == 200:
                    dem_spam_thanh_cong += 1
                    print(f"#{i+1} Spam th√†nh c√¥ng ({phuong_phap}): '{tin_nhan_spam[:50]}...' - Th·ªùi gian ph·∫£n h·ªìi: {response_time:.2f}s")
                    break
                else:
                    print(f"#{i+1} L·ªói kh√¥ng x√°c ƒë·ªãnh. M√£ tr·∫°ng th√°i: {response.status_code}")
                    break

            except requests.exceptions.HTTPError as e:
                dem_bi_chan += 1
                print(f"#{i+1} B·ªä CH·∫∂N HO·∫∂C L·ªñI HTTP: {e} - L·∫ßn th·ª≠ l·∫°i: {retry_count+1}/{max_retries}")
                if e.response.status_code == 429:
                    retry_delay = min(retry_delay * 2, 60)
                    print(f"Ph√°t hi·ªán gi·ªõi h·∫°n t·ªëc ƒë·ªô. TƒÉng th·ªùi gian ch·ªù l√™n {retry_delay} gi√¢y...")
                elif e.response.status_code in [400, 403, 500, 503]:
                    retry_delay = min(retry_delay * 1.5, 30)

            except requests.exceptions.RequestException as e:
                print(f"#{i+1} L·ªñI K·∫æT N·ªêI: {e} - L·∫ßn th·ª≠ l·∫°i: {retry_count+1}/{max_retries}")
                retry_delay = min(retry_delay * 2, 60)

            retry_count += 1
            if retry_count < max_retries:
                time.sleep(retry_delay + random.uniform(0.5, 1.5)) # Randomize delay

        if retry_count == max_retries:
            print(f"#{i+1} ƒê√£ th·ª≠ l·∫°i {max_retries} l·∫ßn nh∆∞ng v·∫´n th·∫•t b·∫°i. Chuy·ªÉn sang l·∫ßn spam ti·∫øp theo.")

        sleep_duration = random.uniform(0.9, 2.8) + response_time * random.uniform(0.1, 0.5) # Delay based on response time
        time.sleep(sleep_duration)

    print(f"Ho√†n th√†nh spam. T·ªïng c·ªông spam th√†nh c√¥ng: {dem_spam_thanh_cong}/{so_lan_spam}. S·ªë l·∫ßn b·ªã ch·∫∑n/l·ªói: {dem_bi_chan}")

async def bat_dau_lenh_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> None:
    user = update.effective_user
    await update.message.reply_markdown_v2(
        fr"Ch√†o {user.mention_markdown_v2()}\! T√¥i l√† Bot Spam NGL t·ªëi th∆∞·ª£ng\.\n\n"
        "S·ª≠ d·ª•ng l·ªánh /spam ƒë·ªÉ b·∫Øt ƒë·∫ßu.",
        reply_markup=telegram.ForceReply(selective=True),
    )

async def spam_lenh_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    await update.message.reply_text('Vui l√≤ng g·ª≠i NGL link b·∫°n mu·ªën spam:')
    return NHAP_LINK_NGL

async def link_ngl_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    link_ngl = update.message.text
    context.user_data['link_ngl'] = link_ngl
    await update.message.reply_text('Tuy·ªát v·ªùi! B√¢y gi·ªù h√£y nh·∫≠p tin nh·∫Øn spam (c√≥ th·ªÉ d√†i d√≤ng):')
    return NHAP_TIN_NHAN_SPAM

async def tin_nhan_spam_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    tin_nhan_spam = update.message.text
    context.user_data['tin_nhan_spam'] = tin_nhan_spam
    await update.message.reply_text('Cu·ªëi c√πng, cho t√¥i bi·∫øt b·∫°n mu·ªën spam bao nhi√™u l·∫ßn:')
    return NHAP_SO_LAN_SPAM

async def so_lan_spam_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    try:
        so_lan_spam = int(update.message.text)
        if so_lan_spam <= 0:
            await update.message.reply_text('S·ªë l·∫ßn spam ph·∫£i l·ªõn h∆°n 0. Vui l√≤ng th·ª≠ l·∫°i.')
            return NHAP_SO_LAN_SPAM
    except ValueError:
        await update.message.reply_text('ƒê·∫ßu v√†o kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p m·ªôt s·ªë.')
        return NHAP_SO_LAN_SPAM

    context.user_data['so_lan_spam'] = so_lan_spam
    await update.message.reply_text('B·∫°n c√≥ mu·ªën s·ª≠ d·ª•ng proxies kh√¥ng? (c√≥/kh√¥ng)')
    return XAC_NHAN_PROXY

async def xac_nhan_proxy_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    xac_nhan = update.message.text.lower()
    if xac_nhan == 'c√≥' or xac_nhan == 'co':
        await update.message.reply_text('Vui l√≤ng t·∫£i l√™n file proxies.txt c·ªßa b·∫°n (m·ªói d√≤ng l√† host:port):')
        return UPLOAD_FILE_PROXY
    elif xac_nhan == 'kh√¥ng' or xac_nhan == 'ko':
        await bat_dau_spam(update, context, proxies=None)
        return ConversationHandler.END
    else:
        await update.message.reply_text('Vui l√≤ng tr·∫£ l·ªùi "c√≥" ho·∫∑c "kh√¥ng".')
        return XAC_NHAN_PROXY

async def upload_file_proxy_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    file_proxy = await update.message.document.get_content()
    proxy_lines = file_proxy.decode().strip().split('\n')
    proxies = [line.strip() for line in proxy_lines if line.strip()]
    if not proxies:
        await update.message.reply_text('File proxy r·ªóng ho·∫∑c kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng. Spam s·∫Ω ti·∫øp t·ª•c kh√¥ng d√πng proxy.')
        proxies = None
    else:
        context.user_data['proxies'] = proxies
        await update.message.reply_text(f'ƒê√£ t·∫£i l√™n {len(proxies)} proxies. B·∫Øt ƒë·∫ßu spam...')

    await bat_dau_spam(update, context, proxies=proxies)
    return ConversationHandler.END

async def bat_dau_spam(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE, proxies=None):
    link_ngl = context.user_data['link_ngl']
    tin_nhan_spam = context.user_data['tin_nhan_spam']
    so_lan_spam = context.user_data['so_lan_spam']

    await update.message.reply_text(f'B·∫Øt ƒë·∫ßu spam {so_lan_spam} l·∫ßn t·ªõi NGL link: {link_ngl}\nS·ª≠ d·ª•ng tin nh·∫Øn: "{tin_nhan_spam[:20]}..."\nS·ª≠ d·ª•ng Proxy: {bool(proxies)}')

    context.job_queue.run_once(
        callback=lambda context: spam_ngl(link_ngl, tin_nhan_spam, so_lan_spam, proxies),
        when=0,
        chat_id=update.effective_chat.id
    )

async def huy_bo_xu_ly(update: telegram.Update, context: CallbackContext.DEFAULT_TYPE) -> int:
    user = update.effective_user
    await update.message.reply_text(
        f"ƒê√£ h·ªßy b·ªè l·ªánh spam, {user.mention_markdown_v2()}!", reply_markup=telegram.ReplyKeyboardRemove()
    )
    return ConversationHandler.END

def main() -> None:
    application = ApplicationBuilder().token(TOKEN_BOT_TELEGRAM).build()

    spam_conversation_handler = ConversationHandler(
        entry_points=[CommandHandler('spam', spam_lenh_xu_ly)],
        states={
            NHAP_LINK_NGL: [MessageHandler(filters.TEXT & ~filters.COMMAND, link_ngl_xu_ly)],
            NHAP_TIN_NHAN_SPAM: [MessageHandler(filters.TEXT & ~filters.COMMAND, tin_nhan_spam_xu_ly)],
            NHAP_SO_LAN_SPAM: [MessageHandler(filters.TEXT & ~filters.COMMAND, so_lan_spam_xu_ly)],
            XAC_NHAN_PROXY: [MessageHandler(filters.TEXT & ~filters.COMMAND, xac_nhan_proxy_xu_ly)],
            UPLOAD_FILE_PROXY: [MessageHandler(filters.Document.MimeType("text/plain"), upload_file_proxy_xu_ly)],
        },
        fallbacks=[CommandHandler('cancel', huy_bo_xu_ly)],
    )

    application.add_handler(CommandHandler("start", bat_dau_lenh_xu_ly))
    application.add_handler(spam_conversation_handler)

    application.run_polling()

if __name__ == "__main__":
    main()