import telegram
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ConversationHandler, CallbackContext
import requests
import time
import random
import os
import json
from fake_useragent import UserAgent
import re

# --- H·∫±ng s·ªë v√† C·∫•u h√¨nh ---
API_URL_NGL = "https://ngl.link/api/submit"
TOKEN_BOT_TELEGRAM = "7766543633:AAFnN9tgGWFDyApzplak0tiJTafCxciFydo"  # **C·∫¶N THAY TH·∫æ B·∫∞NG TOKEN BOT C·ª¶A B·∫†N!**

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 Safari/537.36",
    "Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 Mobile Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (X11; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Edge/120.0.2210.144",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 OPR/106.0.4998.77",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 Brave/120.1.62.129",
    "Mozilla/5.0 (X11; CrOS x86_64 15649.61.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 Safari/537.36",
    "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.129 Mobile Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.119 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.119 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; ASInfo: .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; rv:11.0) like Gecko",
    "Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko"
]
REFERERS = [
    "https://ngl.link/",
    "https://www.google.com/",
    "https://www.facebook.com/",
    "https://www.twitter.com/",
    "https://www.youtube.com/",
    "https://www.instagram.com/",
    "https://www.tiktok.com/",
    "https://www.reddit.com/",
    "https://www.wikipedia.org/",
    "https://vnexpress.net/",
    "https://tuoitre.vn/",
    "https://thanhnien.vn/",
    "https://zingnews.vn/"
]
ACCEPT_ENCODINGS = ["gzip", "deflate", "br", "*"]
ACCEPT_LANGUAGES = ["vi-VN,vi;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5", "en-US,en;q=0.9", "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7", "vi-VN,vi;q=0.9", "en-GB,en;q=0.9"]
CONNECTION_TYPES = ["keep-alive", "close"]
CACHE_CONTROL_DIRECTIVES = ["max-age=0", "no-cache", "no-store", "must-revalidate", "proxy-revalidate", "max-stale=0", "min-fresh=0"]
RETRY_STATUS_CODES = [429, 500, 502, 503, 504]

# --- Tr·∫°ng th√°i h·ªôi tho·∫°i ---
(
    NHAP_LINK_NGL,
    NHAP_TIN_NHAN_SPAM,
    NHAP_SO_LAN_SPAM,
    XAC_NHAN_PROXY,
    UPLOAD_FILE_PROXY
) = range(5)

# --- Ph∆∞∆°ng ph√°p Spam Th√¥ng Minh --- (gi·ªØ nguy√™n)
phuong_phap_spam = [
    "van_ban_thong_thuong", "them_emoji", "tien_to_ky_tu_ngau_nhien", "hau_to_ky_tu_ngau_nhien",
    "hon_hop_chu_hoa_thuong", "leet_speak", "thay_the_ky_tu_dac_biet", "lap_lai_ky_tu",
    "dao_lon_tu_ngu", "thay_the_tu_dong_nghia", "mo_rong_cau", "rut_gon_cau",
    "bien_doi_cau_hoi", "bien_doi_cau_cam_than", "giong_dieu_mia_mai", "giong_dieu_tich_cuc",
    "giong_dieu_tieu_cuc", "giong_dieu_khan_cap", "giong_dieu_to_mo", "khoa_truong", "noi_giam",
    "cau_hoi_tu_tu", "phep_loi_suy_nghi", "an_du", "so_sanh", "nhan_hoa", "gi·ªÖu_c·ª£t", "choi_chu",
    "lap_am_dau", "lap_nguyen_am", "mo_phong_am_thanh", "bao_truoc", "hoi_tuong", "nghi_nghich_ly",
    "cau_tuong_phan", "phan_de", "cao_trao", "ha_trao", "cham_lua", "danh_ngon", "uy·ªÉn_ng·ªØ",
    "c√°ch_noi_khinh_thuong", "c√°ch_noi_thiet_che", "hoan_du", "·∫©n_d·ª•", "ƒëi·ªáp_ng·ªØ_ƒë·∫ßu_c√¢u",
    "ƒëi·ªáp_ng·ªØ_cu·ªëi_c√¢u", "ƒë·∫£o_ng·ªØ", "c√¢u_d·∫´n_li√™n_ti·∫øp", "ƒëa_li√™n_t·ª´", "t·ªânh_l∆∞·ª£c_li√™n_t·ª´"
]

def ap_dung_phuong_phap_spam(tin_nhan, phuong_phap):
    if phuong_phap == "van_ban_thong_thuong":
        return tin_nhan
    elif phuong_phap == "them_emoji":
        emojis = ["üòÇ", "ü§£", "üî•", "üíØ", "üöÄ", "‚ú®", "üåü", "üéâ", "üéä", "üéà"]
        return tin_nhan + " " + random.choice(emojis)
    elif phuong_phap == "tien_to_ky_tu_ngau_nhien":
        tien_to = ''.join(random.choice("abcdefghijklmnopqrstuvwxyz0123456789") for _ in range(random.randint(3, 7)))
        return tien_to + " " + tin_nhan
    elif phuong_phap == "hau_to_ky_tu_ngau_nhien":
        hau_to = ''.join(random.choice("abcdefghijklmnopqrstuvwxyz0123456789") for _ in range(random.randint(3, 7)))
        return tin_nhan + " " + hau_to
    elif phuong_phap == "hon_hop_chu_hoa_thuong":
        return "".join(char.upper() if random.random() < 0.5 else char.lower() for char in tin_nhan)
    elif phuong_phap == "leet_speak":
        leet_chars = {'a': '4', 'e': '3', 'i': '1', 'o': '0', 's': '5', 't': '7', 'l': '1', 'z': '2'}
        return "".join(leet_chars.get(char.lower(), char) for char in tin_nhan)
    elif phuong_phap == "thay_the_ky_tu_dac_biet":
        symbol_chars = {'a': '@', 'b': '8', 'e': '‚Ç¨', 'g': '6', 'h': '#', 'i': '!', 'l': '¬£', 'o': '()', 's': '$', 't': '+', 'z': '2'}
        return "".join(symbol_chars.get(char.lower(), char) for char in tin_nhan)
    elif phuong_phap == "lap_lai_ky_tu":
        if len(tin_nhan) > 3:
            index = random.randint(1, len(tin_nhan) - 2)
            char_to_repeat = tin_nhan[index]
            repeat_count = random.randint(2, 5)
            return tin_nhan[:index] + char_to_repeat * repeat_count + tin_nhan[index+1:]
        return tin_nhan
    elif phuong_phap == "dao_lon_tu_ngu":
        words = tin_nhan.split()
        if len(words) > 2:
            random.shuffle(words[1:-1])
            return " ".join(words)
        return tin_nhan
    elif phuong_phap == "thay_the_tu_dong_nghia":
        synonyms = {
            "t·ªët": ["tuy·ªát v·ªùi", "xu·∫•t s·∫Øc", "phi th∆∞·ªùng", "tuy·ªát di·ªáu"],
            "x·∫•u": ["t·ªìi t·ªá", "kh·ªßng khi·∫øp", "kinh kh·ªßng", "gh√™ s·ª£"],
            "vui": ["h·∫°nh ph√∫c", "sung s∆∞·ªõng", "m√™ ly", "ph·∫•n kh·ªüi"]
        }
        words = tin_nhan.split()
        new_words = []
        for word in words:
            lower_word = word.lower()
            if lower_word in synonyms:
                new_words.append(random.choice(synonyms[lower_word]))
            else:
                new_words.append(word)
        return " ".join(new_words)
    elif phuong_phap == "mo_rong_cau":
        expansions = [
            "Th·ª±c t·∫ø l√†, ", "Tr√™n th·ª±c t·∫ø, ", "Th√†nh th·∫≠t m√† n√≥i, ", "ƒê·ªÉ t√¥i n√≥i cho b·∫°n bi·∫øt, ", "Tin hay kh√¥ng t√πy b·∫°n, "
        ]
        if len(tin_nhan.split()) < 10:
            return random.choice(expansions) + tin_nhan
        return tin_nhan
    elif phuong_phap == "rut_gon_cau":
        contractions = [
            "V·ªÅ c∆° b·∫£n, ", "N√≥i m·ªôt c√°ch ƒë∆°n gi·∫£n, ", "T√≥m l·∫°i, ", "ƒê·ªÉ t·ªïng k·∫øt, "
        ]
        if len(tin_nhan.split()) > 15:
            return random.choice(contractions) + tin_nhan
        return tin_nhan
    elif phuong_phap == "bien_doi_cau_hoi":
        return tin_nhan + "?" if not tin_nhan.endswith("?") else tin_nhan
    elif phuong_phap == "bien_doi_cau_cam_than":
        return tin_nhan + "!" if not tin_nhan.endswith("!") else tin_nhan
    elif phuong_phap == "giong_dieu_mia_mai":
        sarcastic_phrases = ["nh∆∞ th·ªÉ r·ªìi", "·ª´ ƒë√∫ng r·ªìi", "ch·∫Øc ch·∫Øn r·ªìi", "r√µ r√†ng", "t√¥i kh√¥ng nghƒ© v·∫≠y"]
        return tin_nhan + ", " + random.choice(sarcastic_phrases)
    elif phuong_phap == "giong_dieu_tich_cuc":
        positive_phrases = ["Tuy·ªát v·ªùi!", "ƒê·ªânh!", "Phi th∆∞·ªùng!", "Tuy·ªát di·ªáu!", "Xu·∫•t s·∫Øc!"]
        return tin_nhan + " " + random.choice(positive_phrases)
    elif phuong_phap == "giong_dieu_tieu_cuc":
        negative_phrases = ["Th·∫≠t kh√¥ng may", "ƒê√°ng bu·ªìn thay", "Ti·∫øc l√†", "Th·∫≠t x·∫•u h·ªï khi", "Qu√° t·ªá"]
        return random.choice(negative_phrases) + ", " + tin_nhan
    elif phuong_phap == "giong_dieu_khan_cap":
        urgent_phrases = ["Nhanh l√™n!", "Mau l√™n!", "Ngay l·∫≠p t·ª©c!", "ƒê·ª´ng ch·∫≠m tr·ªÖ!", "H√†nh ƒë·ªông ngay!"]
        return random.choice(urgent_phrases) + " " + tin_nhan
    elif phuong_phap == "giong_dieu_to_mo":
        curiosity_phrases = ["ƒêo√°n xem?", "B·∫°n c√≥ bi·∫øt kh√¥ng?", "S·ª± th·∫≠t th√∫ v·ªã:", "Nghe n√†y:", "B·∫°n s·∫Ω kh√¥ng tin ƒë√¢u:"]
        return random.choice(curiosity_phrases) + " " + tin_nhan
    elif phuong_phap == "khoa_truong":
        hyperboles = ["t·ªët nh·∫•t t·ª´ tr∆∞·ªõc ƒë·∫øn nay", "ƒëi·ªÅu tuy·ªát v·ªùi nh·∫•t", "ngo√†i s·ª©c t∆∞·ªüng t∆∞·ª£ng", "kh√≥ tin", "kinh ng·∫°c"]
        return tin_nhan + ", n√≥ l√† " + random.choice(hyperboles) + "!"
    elif phuong_phap == "noi_giam":
        understatements = ["kh√¥ng t·ªá l·∫Øm", "·ªïn th√¥i", "t·∫°m ƒë∆∞·ª£c", "kh√¥ng ph·∫£i t·ªá nh·∫•t", "c√≥ th·ªÉ ch·∫•p nh·∫≠n ƒë∆∞·ª£c"]
        return tin_nhan + ", " + random.choice(understatements)
    elif phuong_phap == "cau_hoi_tu_tu":
        questions = ["ƒê√∫ng kh√¥ng?", "B·∫°n kh√¥ng nghƒ© v·∫≠y sao?", "Kh√¥ng ph·∫£i sao?", "ƒê·ªìng √Ω?", "B·∫°n bi·∫øt m√†?"]
        return tin_nhan + ", " + random.choice(questions) + " ü§î"
    elif phuong_phap == "phep_loi_suy_nghi":
        analogies = [", nh∆∞ m·ªôt...", ", t∆∞∆°ng t·ª± nh∆∞ m·ªôt...", ", nh∆∞ th·ªÉ n√≥ l√† m·ªôt...", ", n√≥ gi·ªëng nh∆∞ m·ªôt..."]
        nouns = ["t√™n l·ª≠a", "d√≤ng s√¥ng", "ng·ªçn n√∫i", "ƒë·∫°i d∆∞∆°ng", "c∆°n b√£o", "gi·∫•c m∆°", "b√†i h√°t"]
        return tin_nhan + analogies[random.randint(0, len(analogies)-1)] + " " + random.choice(nouns)
    elif phuong_phap == "an_du":
        metaphors = ["m·ªôt...", "m·ªôt...", "c·ªßa...", "trong..."]
        nouns = ["ng·ªçn h·∫£i ƒëƒÉng hy v·ªçng", "bi·ªÉn kh·ªï", "v∆∞·ªùn h·∫°nh ph√∫c", "c∆°n b√£o tranh c√£i", "d√≤ng s√¥ng th·ªùi gian"]
        return tin_nhan + ", n√≥ l√† " + random.choice(nouns)
    elif phuong_phap == "so_sanh":
        similes = ["nh∆∞...", "nh∆∞... nh∆∞...", "t∆∞∆°ng t·ª± nh∆∞...", "gi·ªëng v·ªõi..."]
        comparisons = ["m·ªôt ng√¥i sao s√°ng", "m·ªôt con s∆∞ t·ª≠ g·∫ßm r√∫", "m·ªôt l√†n gi√≥ nh·∫π", "m·ªôt ng·ªçn l·ª≠a d·ªØ d·ªôi", "m·ªôt c√°i b√≥ng im l·∫∑ng"]
        return tin_nhan + ", nh∆∞ " + random.choice(comparisons)
    elif phuong_phap == "nhan_hoa":
        personifications = ["Gi√≥ th√¨ th·∫ßm", "M·∫∑t tr·ªùi m·ªâm c∆∞·ªùi", "M∆∞a kh√≥c", "Th·ªùi gian tr√¥i ƒëi", "S·ª± im l·∫∑ng h√©t l√™n"]
        return random.choice(personifications) + ", " + tin_nhan.lower()
    elif phuong_phap == "gi·ªÖu_c·ª£t":
        ironic_phrases = ["√îi, th·∫≠t tuy·ªát v·ªùi", "Qu√° ho√†n h·∫£o", "Ch√≠nh x√°c nh·ªØng g√¨ t√¥i mu·ªën", "Kh√¥ng th·ªÉ t·ªët h∆°n", "Tin tuy·ªát v·ªùi"]
        return random.choice(ironic_phrases) + ", " + tin_nhan
    elif phuong_phap == "choi_chu":
        puns = ["C·∫£i b·∫Øp l·∫≠t c·ªß c·∫£i ƒë∆∞·ªùng!", "B·∫°n c√≥ vui khi g·∫∑p t√¥i kh√¥ng?", "T√¥i th√≠ch ƒë·∫≠u n√†nh b·∫°n!", "Th·ªùi gian tr√¥i nhanh nh∆∞ m·ªôt m≈©i t√™n; ru·ªìi gi·∫•m th√≠ch chu·ªëi.", "B·∫°n g·ªçi m·ªôt con c√° kh√¥ng c√≥ m·∫Øt l√† g√¨? Fsh!"]
        return tin_nhan + ". " + random.choice(puns)
    elif phuong_phap == "lap_am_dau":
        words = tin_nhan.split()
        if len(words) >= 2:
            first_letter = words[0][0].lower()
            alliterative_words = [w for w in words[1:] if w[0].lower() == first_letter]
            if alliterative_words:
                return words[0] + " " + " ".join(alliterative_words) + " v√† " + " ".join([w for w in words[1:] if w[0].lower() != first_letter])
        return tin_nhan
    elif phuong_phap == "lap_nguyen_am":
        vowels = "aeiou"
        words = tin_nhan.split()
        if len(words) >= 2:
            vowel_sound = ""
            for char in words[0].lower():
                if char in vowels:
                    vowel_sound = char
                    break
            if vowel_sound:
                assonated_words = [w for w in words[1:] if vowel_sound in w.lower()]
                if assonated_words:
                    return words[0] + " v√† " + " ".join(assonated_words) + " " + " ".join([w for w in words[1:] if vowel_sound not in w.lower()])
        return tin_nhan
    elif phuong_phap == "mo_phong_am_thanh":
        onomatopoeic_words = ["Bang!", "Boom!", "Clang!", "Hiss!", "Buzz!", "Woof!", "Meow!", "Sizzle!", "Splash!", "Whisper!"]
        return random.choice(onomatopoeic_words) + " " + tin_nhan
    elif phuong_phap == "bao_truoc":
        foreshadowing_phrases = ["H·ªç ƒë√¢u bi·∫øt r·∫±ng...", "Kh√¥ng ai hay bi·∫øt...", "M·∫ßm m·ªëng tai h·ªça ƒë√£ ƒë∆∞·ª£c gieo...", "S·ªë ph·∫≠n ƒë√£ c√≥ nh·ªØng k·∫ø ho·∫°ch kh√°c...", "M·ªôt c∆°n b√£o ƒëang –Ω–∞–∑—Ä–µ–≤–∞–ª–∞..."]
        return random.choice(foreshadowing_phrases) + " " + tin_nhan
    elif phuong_phap == "hoi_tuong":
        flashback_phrases = ["Nhi·ªÅu nƒÉm tr∆∞·ªõc...", "Ng√†y x∆∞a...", "Ng√†y x·ª≠a ng√†y x∆∞a...", "Trong m·ªôt k√Ω ·ª©c xa xƒÉm...", "T·ª´ s√¢u th·∫≥m qu√° kh·ª©..."]
        return random.choice(flashback_phrases) + ", " + tin_nhan
    elif phuong_phap == "nghi_nghich_ly":
        paradoxes = ["S·ª± im l·∫∑ng th·∫≠t ƒëi·∫øc tai.", "√çt h∆°n l√† nhi·ªÅu h∆°n.", "T√¥i ph·∫£i t√†n nh·∫´n ƒë·ªÉ t·ª≠ t·∫ø.", "K·∫ª ng·ªëc kh√¥n ngoan.", "V·ª´a ng·ªçt v·ª´a ƒë·∫Øng."]
        return tin_nhan + ". " + random.choice(paradoxes)
    elif phuong_phap == "cau_tuong_phan":
        oxymorons = ["ng·ªçt ƒë·∫Øng", "x√°c s·ªëng", "im l·∫∑ng ƒëi·∫øc tai", "t√¥m jumbo", "h·ªón lo·∫°n c√≥ t·ªï ch·ª©c"]
        return tin_nhan + ", nh∆∞ m·ªôt " + random.choice(oxymorons)
    elif phuong_phap == "phan_de":
        antitheses = ["Ng∆∞·ªùi t√≠nh kh√¥ng b·∫±ng tr·ªùi t√≠nh.", "L·ªùi n√≥i l√† b·∫°c, im l·∫∑ng l√† v√†ng.", "D·ªÖ ƒë·∫øn d·ªÖ ƒëi.", "Nh√¢n v√¥ th·∫≠p to√†n.", "H√£y l·∫Øng nghe t·∫•t c·∫£ m·ªçi ng∆∞·ªùi, nh∆∞ng ch·ªâ n√≥i v·ªõi s·ªë √≠t."]
        return tin_nhan + ". " + random.choice(antitheses)
    elif phuong_phap == "cao_trao":
        climax_phrases = ["v√† ƒëi·ªÅu quan tr·ªçng nh·∫•t l√†...", "tr√™n h·∫øt...", "ƒëi·ªÉm m·∫•u ch·ªët l√†...", "ƒë·ªânh cao c·ªßa t·∫•t c·∫£...", "k·∫øt th√∫c ho√†nh tr√°ng l√†..."]
        return tin_nhan + ", " + random.choice(climax_phrases)
    elif phuong_phap == "ha_trao":
        anticlimaxes = ["nh∆∞ng sau ƒë√≥ kh√¥ng c√≥ g√¨ x·∫£y ra.", "v√† n√≥ kh√° l√† g√¢y th·∫•t v·ªçng.", "v√† n√≥ ch·ªâ... ·ªïn th√¥i.", "nh∆∞ng h√≥a ra n√≥ h∆°i g√¢y th·∫•t v·ªçng.", "v√† ti·∫øt l·ªô l·ªõn l√†... meh."]
        return tin_nhan + ", " + random.choice(anticlimaxes)
    elif phuong_phap == "cham_lua":
        return tin_nhan + "..."
    elif phuong_phap == "danh_ngon":
        epigrams = ["Sai l·∫ßm l√† c·ªßa con ng∆∞·ªùi, nh∆∞ng ƒë·ªÉ th·ª±c s·ª± l√†m r·ªëi tung m·ªçi th·ª© c·∫ßn ƒë·∫øn m√°y t√≠nh.", "T√¥i c√≥ th·ªÉ c∆∞·ª°ng l·∫°i m·ªçi th·ª© tr·ª´ s·ª± c√°m d·ªó.", "C√°ch duy nh·∫•t ƒë·ªÉ lo·∫°i b·ªè s·ª± c√°m d·ªó l√† ƒë·∫ßu h√†ng n√≥.", "H√£y l√† ch√≠nh m√¨nh; m·ªçi ng∆∞·ªùi kh√°c ƒë√£ c√≥ ng∆∞·ªùi kh√°c l√†m r·ªìi.", "S·ª± th·∫≠t hi·∫øm khi thu·∫ßn khi·∫øt v√† kh√¥ng bao gi·ªù ƒë∆°n gi·∫£n."]
        return tin_nhan + ". " + random.choice(epigrams)
    elif phuong_phap == "uy·ªÉn_ng·ªØ":
        euphemisms = ["qua ƒë·ªùi", "kh√≥ khƒÉn v·ªÅ kinh t·∫ø", "khuy·∫øt t·∫≠t", "c∆° s·ªü c·∫£i hu·∫•n", "ƒë√£ qua s·ª≠ d·ª•ng"]
        original_words = ["ch·∫øt", "ngh√®o", "t√†n t·∫≠t", "t√π", "ƒë√£ d√πng"]
        word_pairs = list(zip(original_words, euphemisms))
        for original, euphemism in word_pairs:
            tin_nhan = tin_nhan.replace(original, euphemism, 1)
        return tin_nhan
    elif phuong_phap == "c√°ch_noi_khinh_thuong":
        litotes_phrases = ["kh√¥ng t·ªá", "kh√¥ng ph·∫£i kh√¥ng gi·ªëng", "kh√¥ng ph·∫£i kh√¥ng ƒë√°ng k·ªÉ", "kh√¥ng ph·∫£i kh√¥ng ph·ªï bi·∫øn", "kh√¥ng ph·∫£i m·ªôt ch√∫t"]
        return tin_nhan + ", n√≥ " + random.choice(litotes_phrases)
    elif phuong_phap == "c√°ch_noi_thiet_che":
        meiosis_phrases = ["ch·ªâ l√† m·ªôt v·∫øt x∆∞·ªõc", "h∆°i h∆°i", "m·ªôt ch√∫t x√≠u", "m·ªôt s·ª± c·ªë nh·ªè", "m·ªôt ch√∫t b·∫•t ti·ªán"]
        return tin_nhan + ", n√≥ ch·ªâ l√† " + random.choice(meiosis_phrases)
    elif phuong_phap == "hoan_du":
        synecdoches = ["b√°nh xe", "s·ª£i ch·ªâ", "gi√†y ·ªëng tr√™n m·∫∑t ƒë·∫•t", "b·ªô com l√™", "v∆∞∆°ng mi·ªán"]
        full_meanings = ["xe h∆°i", "qu·∫ßn √°o", "binh l√≠nh", "doanh nh√¢n", "ch·∫ø ƒë·ªô qu√¢n ch·ªß"]
        word_pairs = list(zip(synecdoches, full_meanings))
        for synecdoche, full_meaning in word_pairs:
            tin_nhan = tin_nhan.replace(full_meaning, synecdoche, 1)
        return tin_nhan
    elif phuong_phap == "·∫©n_d·ª•":
        metonymies = ["Nh√† Tr·∫Øng", "Ph·ªë Wall", "Hollywood", "Thung l≈©ng Silicon", "Ph·ªë Fleet"]
        full_meanings = ["T·ªïng th·ªëng M·ªπ", "c√°c t·ªï ch·ª©c t√†i ch√≠nh", "ng√†nh c√¥ng nghi·ªáp ƒëi·ªán ·∫£nh M·ªπ", "ng√†nh c√¥ng nghi·ªáp c√¥ng ngh·ªá M·ªπ", "b√°o ch√≠ Anh"]
        word_pairs = list(zip(metonymies, full_meanings))
        for metonymy, full_meaning in word_pairs:
            tin_nhan = tin_nhan.replace(full_meaning, metonymy, 1)
        return tin_nhan
    elif phuong_phap == "ƒëi·ªáp_ng·ªØ_ƒë·∫ßu_c√¢u":
        prefixes = ["H√£y nh·ªõ r·∫±ng, ", "ƒê·ª´ng bao gi·ªù qu√™n, ", "H√£y ghi nh·ªõ, ", "Lu√¥n xem x√©t, ", "Ch√∫ng ta ƒë·ª´ng b·ªè qua, "]
        return random.choice(prefixes) + tin_nhan
    elif phuong_phap == "ƒëi·ªáp_ng·ªØ_cu·ªëi_c√¢u":
        suffixes = [", h√£y nh·ªõ ƒëi·ªÅu ƒë√≥", ", h√£y ghi nh·ªõ ƒëi·ªÅu ƒë√≥", ", ƒë·ª´ng bao gi·ªù qu√™n", ", lu√¥n xem x√©t ƒëi·ªÅu ƒë√≥", ", ch√∫ng ta ƒë·ª´ng b·ªè qua ƒëi·ªÅu ƒë√≥"]
        return tin_nhan + random.choice(suffixes)
    elif phuong_phap == "ƒë·∫£o_ng·ªØ":
        words = tin_nhan.split()
        if len(words) >= 4:
            return words[0] + " " + words[1] + " " + words[-2] + " " + words[-1]
        return tin_nhan
    elif phuong_phap == "c√¢u_d·∫´n_li√™n_ti·∫øp":
        verbs = ["chinh ph·ª•c", "ph√° h·ªßy", "x√¢y d·ª±ng", "t·∫°o ra", "m·∫•t m√°t"]
        nouns1 = ["tr√°i tim", "t√¢m tr√≠", "v·∫≠n may", "ƒë·∫ø ch·∫ø", "gi·∫•c m∆°"]
        nouns2 = ["th√†nh ph·ªë", "qu·ªëc gia", "v∆∞∆°ng qu·ªëc", "th·∫ø gi·ªõi", "thi√™n h√†"]
        return random.choice(verbs).capitalize() + " " + random.choice(nouns1) + " v√† " + random.choice(nouns2) + "."
    elif phuong_phap == "ƒëa_li√™n_t·ª´":
        words = tin_nhan.split()
        if len(words) >= 3:
            return " v√† ".join(words)
        return tin_nhan
    elif phuong_phap == "t·ªânh_l∆∞·ª£c_li√™n_t·ª´":
        words = tin_nhan.split()
        if len(words) >= 3:
            return ", ".join(words[:-1]) + " " + words[-1]
        return tin_nhan
    else:
        return tin_nhan

def tao_headers_nguoi_dung():
    ua = UserAgent()
    try:
        user_agent = ua.random
    except Exception:
        user_agent = random.choice(USER_AGENTS)

    headers = {
        'User-Agent': user_agent,
        'Referer': random.choice(REFERERS),
        'Accept-Encoding': random.choice(ACCEPT_ENCODINGS),
        'Accept-Language': random.choice(ACCEPT_LANGUAGES),
        'Connection': random.choice(CONNECTION_TYPES),
        'Cache-Control': random.choice(CACHE_CONTROL_DIRECTIVES),
        'Upgrade-Insecure-Requests': '1',
        'Origin': 'https://ngl.link',
        'TE': 'Trailers',
        'Sec-Fetch-Site': 'same-origin',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Ch-Ua': '"Not_A Brand";v="99", "Google Chrome";v="120", "Chromium";v="120"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"Windows"',
    }
    if random.random() < 0.3:
        headers['DNT'] = '1'
    if random.random() < 0.2:
        headers['Sec-GPC'] = '1'
    if random.random() < 0.1:
        del headers['Cache-Control']
    header_items = list(headers.items())
    random.shuffle(header_items)
    return dict(header_items)

def tao_thoi_gian_tre_nguoi_dung():
    return random.expovariate(2.0) + 0.5

def spam_ngl(ngl_link, tin_nhan, so_lan_spam, proxies=None):
    print(f"B·∫Øt ƒë·∫ßu spam NGL: {ngl_link}, Tin nh·∫Øn: '{tin_nhan}', S·ªë l·∫ßn: {so_lan_spam}, Proxies: {bool(proxies)}")
    dem_spam_thanh_cong = 0
    dem_bi_chan = 0
    session = requests.Session()
    retry_strategy = requests.packages.urllib3.util.retry.Retry(
        total=5,
        status_forcelist=RETRY_STATUS_CODES,
        backoff_factor=2,
        method_whitelist=["POST"]
    )
    adapter = requests.adapters.HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    for i in range(so_lan_spam):
        phuong_phap = random.choice(phuong_phap_spam)
        tin_nhan_spam = ap_dung_phuong_phap_spam(tin_nhan, phuong_phap)

        du_lieu = {
            "link": ngl_link.split("/")[-1],
            "question": tin_nhan_spam
        }

        headers = tao_headers_nguoi_dung()
        proxy = None
        if proxies:
            proxy_str = random.choice(proxies)
            if proxy_str:
                proxy_host, proxy_port = proxy_str.split(":")
                proxy = {'http': f'http://{proxy_host}:{proxy_port}', 'https': f'https://{proxy_host}:{proxy_port}'}

        try:
            time.sleep(tao_thoi_gian_tre_nguoi_dung() / 2)
            response = session.post(API_URL_NGL, data=du_lieu, headers=headers, proxies=proxy, timeout=20)
            response.raise_for_status()
            if response.status_code == 200:
                dem_spam_thanh_cong += 1
                print(f"#{i+1} Spam th√†nh c√¥ng ({phuong_phap}): '{tin_nhan_spam[:50]}...'")
            else:
                print(f"#{i+1} L·ªói kh√¥ng x√°c ƒë·ªãnh. M√£ tr·∫°ng th√°i: {response.status_code}")
        except requests.exceptions.HTTPError as e:
            dem_bi_chan += 1
            print(f"#{i+1} B·ªä CH·∫∂N HO·∫∂C L·ªñI HTTP: {e}")
            if e.response.status_code == 429:
                print("Ph√°t hi·ªán gi·ªõi h·∫°n t·ªëc ƒë·ªô. T·∫°m d·ª´ng spam l√¢u h∆°n...")
                time.sleep(30 + random.randint(10, 20))
            elif e.response.status_code in [400, 403, 500, 503]:
                time.sleep(5 + random.random())
        except requests.exceptions.RequestException as e:
            print(f"#{i+1} L·ªñI K·∫æT N·ªêI: {e}")
            time.sleep(5 + random.random())

        time.sleep(tao_thoi_gian_tre_nguoi_dung())

    print(f"Ho√†n th√†nh spam. T·ªïng c·ªông spam th√†nh c√¥ng: {dem_spam_thanh_cong}/{so_lan_spam}. S·ªë l·∫ßn b·ªã ch·∫∑n/l·ªói: {dem_bi_chan}")

async def bat_dau_lenh_xu_ly(update: telegram.Update, context: CallbackContext) -> None:
    user = update.effective_user
    await update.message.reply_markdown_v2(
        fr"Ch√†o {user.mention_markdown_v2()}! T√¥i l√† Bot Spam NGL **T·ªëi Th∆∞·ª£ng 2025 - Kh√¥ng D·∫•u V·∫øt**.\n\n"
        "S·ª≠ d·ª•ng l·ªánh /help ƒë·ªÉ kh√°m ph√° s·ª©c m·∫°nh v√¥ song.",
        reply_markup=telegram.ForceReply(selective=True),
    )

async def help_lenh_xu_ly(update: telegram.Update, context: CallbackContext) -> None:
    help_text = r"""
**Bot Spam NGL T·ªëi Th∆∞·ª£ng 2025 - Kh√¥ng D·∫•u V·∫øt - H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng**

Ch√†o m·ª´ng ƒë·∫øn v·ªõi ƒë·ªânh cao c√¥ng ngh·ªá spam NGL! Bot n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ v∆∞·ª£t qua m·ªçi gi·ªõi h·∫°n, v·ªõi kh·∫£ nƒÉng ch·ªëng ch·∫∑n v√¥ song v√† x√≥a d·∫•u v·∫øt ho√†n h·∫£o. H√£y kh√°m ph√° s·ª©c m·∫°nh th·ª±c s·ª±:

**C√°c L·ªánh:**

*   `/start`: Kh·ªüi ƒë·ªông bot v√† ngh√™nh ƒë√≥n s·ª©c m·∫°nh t·ªëi th∆∞·ª£ng.
*   `/help`: M·ªü ra c·∫©m nang h∆∞·ªõng d·∫´n to√†n di·ªán n√†y.
*   `/spam`: K√≠ch ho·∫°t quy tr√¨nh spam NGL. Bot s·∫Ω d·∫´n d·∫Øt b·∫°n t·ª´ng b∆∞·ªõc.
*   `/cancel`: Tho√°t kh·ªèi quy tr√¨nh spam b·∫•t c·ª© l√∫c n√†o.

**Quy Tr√¨nh Spam (l·ªánh `/spam`):**

1.  **G√µ `/spam`:**  B·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán v√† chu·∫©n b·ªã cho h√†nh ƒë·ªông.
2.  **NGL Link M·ª•c Ti√™u:**  Cung c·∫•p URL NGL (v√≠ d·ª•: `https://ngl.link/username`).
3.  **Tin Nh·∫Øn B√£o T√°:** Nh·∫≠p n·ªôi dung tin nh·∫Øn. Bot s·∫Ω t·ª± ƒë·ªông bi·∫øn h√≥a n√≥ v·ªõi **50 ph∆∞∆°ng ph√°p spam si√™u vi·ªát**, ƒë·∫£m b·∫£o s·ª± ƒë·ªôc ƒë√°o v√† kh√≥ b·ªã ph√°t hi·ªán.
4.  **S·ªë L∆∞·ª£ng √Åp ƒê·∫£o:**  Ch·ªçn s·ªë l·∫ßn spam. H√£y nh·ªõ, s·ª©c m·∫°nh h·ªßy di·ªát n·∫±m trong tay b·∫°n.
5.  **Proxy T√†ng H√¨nh (T√πy Ch·ªçn, nh∆∞ng C·ª±c K·ª≥ Khuy·∫øn Ngh·ªã):**
    *   **Ch·ªçn "c√≥":** T·∫£i l√™n file `proxies.txt`. File n√†y ph·∫£i l√† file vƒÉn b·∫£n thu·∫ßn t√∫y, **m·ªói d√≤ng ch·ª©a m·ªôt proxy theo ƒë·ªãnh d·∫°ng `host:port`** (v√≠ d·ª•: `123.45.67.89:8080`). **ƒê·∫£m b·∫£o m·ªói d√≤ng ch·ªâ ch·ª©a m·ªôt proxy duy nh·∫•t v√† ƒë√∫ng ƒë·ªãnh d·∫°ng.** Proxy l√† **l√° ch·∫Øn t√†ng h√¨nh**, gi√∫p b·∫°n ·∫©n danh v√† v∆∞·ª£t t∆∞·ªùng l·ª≠a. **Proxy ch·∫•t l∆∞·ª£ng cao = s·ª©c m·∫°nh t·ªëi th∆∞·ª£ng.**
    *   **Ch·ªçn "kh√¥ng":** Spam kh√¥ng proxy (√≠t ƒë∆∞·ª£c khuy·∫øn kh√≠ch cho s·ªë l∆∞·ª£ng l·ªõn).
6.  **B·∫Øt ƒê·∫ßu Cu·ªôc T·∫•n C√¥ng:**  X√°c nh·∫≠n th√¥ng tin. Bot s·∫Ω gi·∫£i ph√≥ng s·ª©c m·∫°nh spam trong n·ªÅn, th√¥ng b√°o k·∫øt qu·∫£ v√† ti·∫øn tr√¨nh.

**T√≠nh NƒÉng T·ªëi Th∆∞·ª£ng 2025 - Kh√¥ng D·∫•u V·∫øt:**

*   **50 Ph∆∞∆°ng Ph√°p Spam Si√™u Vi·ªát:**  Bi·∫øn h√≥a tin nh·∫Øn kh√¥ng gi·ªõi h·∫°n, th√°ch th·ª©c m·ªçi b·ªô l·ªçc.
*   **Gi·∫£ L·∫≠p H√†nh vi Ng∆∞·ªùi D√πng Ho√†n H·∫£o (Human-like Interaction Bypass):** (Nh∆∞ tr∆∞·ªõc)
*   **Ch·ªëng Ch·∫∑n B·∫≠c Nh·∫•t:** (Nh∆∞ tr∆∞·ªõc)
*   **X√≥a D·∫•u V·∫øt Tuy·ªát ƒê·ªëi:** (Nh∆∞ tr∆∞·ªõc)
*   **Th∆∞ Vi·ªán v√† C√¥ng Ngh·ªá V∆∞·ª£t Tr·ªôi:** (Nh∆∞ tr∆∞·ªõc)

**C·∫£nh B√°o:** (Nh∆∞ tr∆∞·ªõc)
*   **ƒê·ªãnh d·∫°ng file proxy:** **File `proxies.txt` ph·∫£i l√† file vƒÉn b·∫£n thu·∫ßn t√∫y, m·ªói d√≤ng ch·ª©a m·ªôt proxy theo ƒë·ªãnh d·∫°ng `host:port`. ƒê·∫£m b·∫£o kh√¥ng c√≥ k√Ω t·ª± l·∫°, kho·∫£ng tr·∫Øng th·ª´a ƒë·∫ßu d√≤ng/cu·ªëi d√≤ng, ho·∫∑c d√≤ng tr·ªëng kh√¥ng mong mu·ªën.**

H√£y l√†m ch·ªß s·ª©c m·∫°nh n√†y. S·ª≠ d·ª•ng bot m·ªôt c√°ch kh√¥n ngoan v√† c√≥ tr√°ch nhi·ªám. M·ªçi th·∫Øc m·∫Øc, d√πng l·ªánh `/help` ho·∫∑c li√™n h·ªá nh√† ph√°t tri·ªÉn (t·ª©c l√† t√¥i!).

**Ch√∫c b·∫°n th√†nh c√¥ng trong m·ªçi nhi·ªám v·ª•!**
    """
    await update.message.reply_markdown_v2(help_text, reply_markup=telegram.ReplyKeyboardRemove())

async def spam_lenh_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    await update.message.reply_text('Vui l√≤ng g·ª≠i NGL link m·ª•c ti√™u:')
    return NHAP_LINK_NGL

async def link_ngl_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    link_ngl = update.message.text
    context.user_data['link_ngl'] = link_ngl
    await update.message.reply_text('Tuy·ªát v·ªùi! H√£y nh·∫≠p tin nh·∫Øn b√£o t√°p b·∫°n mu·ªën g·ª≠i:')
    return NHAP_TIN_NHAN_SPAM

async def tin_nhan_spam_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    tin_nhan_spam = update.message.text
    context.user_data['tin_nhan_spam'] = tin_nhan_spam
    await update.message.reply_text('S·ªë l·∫ßn spam √°p ƒë·∫£o b·∫°n mong mu·ªën l√† bao nhi√™u?')
    return NHAP_SO_LAN_SPAM

async def so_lan_spam_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    try:
        so_lan_spam = int(update.message.text)
        if so_lan_spam <= 0:
            await update.message.reply_text('S·ªë l·∫ßn spam ph·∫£i l·ªõn h∆°n 0. H√£y th·ª≠ l·∫°i.')
            return NHAP_SO_LAN_SPAM
    except ValueError:
        await update.message.reply_text('ƒê·∫ßu v√†o kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p m·ªôt s·ªë.')
        return NHAP_SO_LAN_SPAM

    context.user_data['so_lan_spam'] = so_lan_spam
    await update.message.reply_text('B·∫°n c√≥ c·∫ßn **Proxy T√†ng H√¨nh** ƒë·ªÉ b·∫£o v·ªá danh t√≠nh kh√¥ng? (c√≥/kh√¥ng)')
    return XAC_NHAN_PROXY

async def xac_nhan_proxy_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    xac_nhan = update.message.text.lower()
    if xac_nhan == 'c√≥' or xac_nhan == 'co':
        await update.message.reply_text('Tuy·ªát v·ªùi! H√£y t·∫£i l√™n file `proxies.txt` c·ªßa b·∫°n (m·ªói d√≤ng `host:port`):')
        return UPLOAD_FILE_PROXY
    elif xac_nhan == 'kh√¥ng' or xac_nhan == 'ko':
        await bat_dau_spam(update, context, proxies=None)
        return ConversationHandler.END
    else:
        await update.message.reply_text('C√¢u tr·∫£ l·ªùi kh√¥ng h·ª£p l·ªá. H√£y ch·ªçn "c√≥" ho·∫∑c "kh√¥ng".')
        return XAC_NHAN_PROXY

async def upload_file_proxy_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    valid_proxies = []
    invalid_lines_count = 0
    error_message = ""

    try:
        file_proxy_content = await update.message.document.get_content()
        proxy_file_text = file_proxy_content.decode('utf-8')  # Th·ª≠ decode UTF-8 tr∆∞·ªõc
    except UnicodeDecodeError:
        try:
            proxy_file_text = file_proxy_content.decode('latin-1')  # N·∫øu UTF-8 l·ªói, th·ª≠ latin-1
        except UnicodeDecodeError:
            proxy_file_text = ""  # N·∫øu c·∫£ hai ƒë·ªÅu l·ªói, coi nh∆∞ file r·ªóng v√† b√°o l·ªói

    proxy_lines = proxy_file_text.strip().split('\n')

    for line_number, line in enumerate(proxy_lines, start=1):  # B·∫Øt ƒë·∫ßu ƒë·∫øm d√≤ng t·ª´ 1
        line = line.strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu v√† cu·ªëi d√≤ng
        if not line:  # B·ªè qua d√≤ng tr·ªëng
            continue

        proxy_parts = line.split(':')
        if len(proxy_parts) == 2:
            host, port_str = proxy_parts
            if re.match(r"^\d+$", port_str):  # Ki·ªÉm tra port c√≥ ph·∫£i l√† s·ªë kh√¥ng d√πng regex
                try:
                    port = int(port_str)
                    valid_proxies.append(f"{host}:{port}")  # L∆∞u proxy h·ª£p l·ªá
                except ValueError:
                    error_message += f"D√≤ng {line_number}: Port kh√¥ng h·ª£p l·ªá (kh√¥ng ph·∫£i s·ªë).\n"
                    invalid_lines_count += 1
            else:
                error_message += f"D√≤ng {line_number}: Port kh√¥ng h·ª£p l·ªá (kh√¥ng ph·∫£i s·ªë).\n"
                invalid_lines_count += 1
        else:
            error_message += f"D√≤ng {line_number}: ƒê·ªãnh d·∫°ng proxy kh√¥ng h·ª£p l·ªá (ph·∫£i l√† host:port).\n"
            invalid_lines_count += 1

    if invalid_lines_count > 0:
        error_message = f"**C·∫£nh b√°o: Ph√°t hi·ªán l·ªói trong file proxy:**\n" + error_message + f"\n**T·ªïng c·ªông {invalid_lines_count} d√≤ng kh√¥ng h·ª£p l·ªá ƒë√£ b·ªã b·ªè qua.**\nCh·ªâ s·ª≠ d·ª•ng c√°c proxy h·ª£p l·ªá.\n"

    if valid_proxies:
        context.user_data['proxies'] = valid_proxies
        success_message = f"**ƒê√£ t·∫£i l√™n {len(valid_proxies)} Proxy T√†ng H√¨nh h·ª£p l·ªá**."
        if invalid_lines_count > 0:
            success_message += "\n" + error_message
        else:
            success_message += "\nChu·∫©n b·ªã cho cu·ªôc t·∫•n c√¥ng..."
        await update.message.reply_markdown_v2(success_message)

    else:
        context.user_data['proxies'] = None
        no_proxy_message = "**File proxy kh√¥ng c√≥ proxy h·ª£p l·ªá n√†o ƒë∆∞·ª£c t√¨m th·∫•y.**\n"
        if invalid_lines_count > 0:
            no_proxy_message += error_message + "\n"  # Th√™m th√¥ng b√°o l·ªói n·∫øu c√≥ d√≤ng kh√¥ng h·ª£p l·ªá
        no_proxy_message += "Spam s·∫Ω di·ªÖn ra **KH√îNG** s·ª≠ d·ª•ng proxy (k√©m an to√†n h∆°n v√† d·ªÖ b·ªã ch·∫∑n)."
        await update.message.reply_markdown_v2(no_proxy_message)

    await bat_dau_spam(update, context, proxies=valid_proxies if valid_proxies else None)  # Truy·ªÅn None n·∫øu kh√¥ng c√≥ proxy h·ª£p l·ªá
    return ConversationHandler.END


async def bat_dau_spam(update: telegram.Update, context: CallbackContext, proxies=None):
    link_ngl = context.user_data['link_ngl']
    tin_nhan_spam = context.user_data['tin_nhan_spam']
    so_lan_spam = context.user_data['so_lan_spam']

    await update.message.reply_text(f'**KH·ªûI ƒê·ªòNG SPAM B√ÉO T√ÅP!**\n\n m·ª•c ti√™u: `{link_ngl}`\n Tin nh·∫Øn: `{tin_nhan_spam[:20]}...`\n S·ªë l·∫ßn: `{so_lan_spam}`\n Proxy T√†ng H√¨nh: `{bool(proxies)}`\n\n**S·ª©c m·∫°nh t·ªëi th∆∞·ª£ng ƒë√£ ƒë∆∞·ª£c gi·∫£i ph√≥ng...**')

    context.job_queue.run_once(
        callback=spam_callback,
        when=0,
        chat_id=update.effective_chat.id,
        data={'ngl_link': link_ngl, 'tin_nhan_spam': tin_nhan_spam, 'so_lan_spam': so_lan_spam, 'proxies': proxies}
    )

async def spam_callback(context: CallbackContext):
    job_data = context.job.data
    spam_ngl(job_data['ngl_link'], job_data['tin_nhan_spam'], job_data['so_lan_spam'], job_data['proxies'])

async def huy_bo_xu_ly(update: telegram.Update, context: CallbackContext) -> int:
    user = update.effective_user
    await update.message.reply_text(
        f"**ƒê√£ h·ªßy b·ªè l·ªánh spam.** S·ª©c m·∫°nh ƒë√£ ƒë∆∞·ª£c thu h·ªìi, {user.mention_markdown_v2()}!", reply_markup=telegram.ReplyKeyboardRemove()
    )
    return ConversationHandler.END

def main() -> None:
    application = Application.builder().token(TOKEN_BOT_TELEGRAM).build()

    spam_conversation_handler = ConversationHandler(
        entry_points=[CommandHandler('spam', spam_lenh_xu_ly)],
        states={
            NHAP_LINK_NGL: [MessageHandler(filters.TEXT & ~filters.COMMAND, link_ngl_xu_ly)],
            NHAP_TIN_NHAN_SPAM: [MessageHandler(filters.TEXT & ~filters.COMMAND, tin_nhan_spam_xu_ly)],
            NHAP_SO_LAN_SPAM: [MessageHandler(filters.TEXT & ~filters.COMMAND, so_lan_spam_xu_ly)],
            XAC_NHAN_PROXY: [MessageHandler(filters.TEXT & ~filters.COMMAND, xac_nhan_proxy_xu_ly)],
            UPLOAD_FILE_PROXY: [MessageHandler(filters.Document.MimeType("text/plain"), upload_file_proxy_xu_ly)],
        },
        fallbacks=[CommandHandler('cancel', huy_bo_xu_ly)],
    )

    application.add_handler(CommandHandler("start", bat_dau_lenh_xu_ly))
    application.add_handler(CommandHandler("help", help_lenh_xu_ly))
    application.add_handler(spam_conversation_handler)

    application.run_polling()

if __name__ == "__main__":
    main()